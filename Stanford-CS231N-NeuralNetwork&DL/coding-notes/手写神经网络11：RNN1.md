# 手写神经网络11：RNN

> 这个系列是我在完成斯坦福公开课CS231N的作业时候所做的一些记录，这门课程是公认的深度学习入门的神课，其作业也非常硬核，需要从底层实现各种神经网络模型，包括前馈神经网络、卷积神经网络和循环神经网络，LSTM等等。
>
> 作业不仅要求掌握numpy等python库的api用法，更要对神经网络的数学理论和公式推导有非常深的理解，实现起来难度比较大，因此我也将在这个系列的博客中记录自己推导和coding的过程
>
> 限于个人水平实在有限，我尽量减少参考网上代码的次数

## 1.RNN的结构

- RNN指的是循环神经网络，是一种设计出来用于**处理序列化数据(比如自然语言，时序数据)的神经网络**，RNN可以保留一定的上下文信息并将序列前面的信息不断向后面传递。
- RNN的架构如下图所示，主要包含一个输入层，一个隐层和一个输出层，隐层单元的隐状态$h_t$也会不断向下一个隐层单元传递。

<img src="static/image-20210517185038311.png" alt="RNN架构图" style="zoom:50%;" />

- RNN的隐状态更新公式可以表示为：

$$
h_t=\sigma(W_xx_{t}+W_hh_{t-1}+b)
$$

- 这里的**可学习参数**包括$W_x,W_h,b$，分别是两个权重矩阵和一个bias向量，而激活函数通常使用双曲正切tanh函数，而最终的输出结果的计算方式是：

$$
y_t=\mathrm{softmax}(W_sh_t)
$$

## 2.RNN的前向传播及实现

- 从RNN的架构可以看出，我们输入RNN的数据是一个序列$X=(x_1,x_2,\dots,x_T)$，这个序列需要在RNN中按照顺序逐渐前向传播，最终得到一个隐状态的序列$H=(h_1,h_2,\dots,h_T)$，而每一个单元内的前向传播过程可以用函数`rnn_step_forward`来描述

```python
def rnn_step_forward(x, prev_h, Wx, Wh, b):
    """
    The input data has dimension D, the hidden state has dimension H,
    and the minibatch is of size N.
    Inputs:
    - x: Input data for this timestep, of shape (N, D)
    - prev_h: Hidden state from previous timestep, of shape (N, H)
    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
    - b: Biases of shape (H,)
    Returns a tuple of:
    - next_h: Next hidden state, of shape (N, H)
    - cache: Tuple of values needed for the backward pass.
    """
    next_h = np.tanh(np.matmul(prev_h, Wh) + np.matmul(x, Wx) + b)
    cache = (x, prev_h, Wx, Wh, b, next_h)
    return next_h, cache
```

- 而整个序列的前向传播需要使用一个循环来完成，因此**RNN的训练不能并行化**，这也是RNN的一个重大缺点。我们使用一个函数来描述整个序列在RNN中的前向传播过程：

```python
def rnn_forward(x, h0, Wx, Wh, b):
    """
    We assume an input sequence composed of T vectors, each of dimension D.
    The RNN uses a hidden size of H, and we work over a minibatch containing
    N sequences. After running the RNN forward,
    Inputs:
    - x: Input data for the entire time series, of shape (N, T, D)
    - h0: Initial hidden state, of shape (N, H)
    """
    h, cache = None, []
    N, T, D = x.shape
    N, H = h0.shape
    # 最终生成的一个隐状态
    h = np.zeros((N, T, H))
    next_h = h0
    for i in range(T):
        x_step = x[:, i, :].reshape(N, D)
        next_h, cache_step = rnn_step_forward(x_step, next_h, Wx, Wh, b)
        h[:, i, :] = next_h
        # cache用来保存每个单元中的有效信息，用于反向传播过程中的导数计算
        cache.append(cache_step)
    return h, cache
```

- RNN的前向传播总体来说比较简单。

## 3.RNN的反向传播及实现

- RNN的反向传播和CNN等传统神经网络不同，是一种**“时间”上的反向传播**，也就是说在计算梯度并更新参数的时候，不仅要考虑从最上层的loss函数中传递下来的梯度，**也要考虑从后面一个隐层单元(从前向传播的时间来看要更迟，所以被称为时间反向传播)传递下来的梯度**
- 而对于各个参数，其梯度的计算方式如下(注意tanh函数导数的特殊性)：

$$
s_t=W_xx_{t}+W_hh_{t-1}+b,\quad h_t=\tanh(s_t)
$$

$$
\frac{\partial h_t}{\partial s_t}=1-\tanh^2(s_t)
$$

$$
\frac{\partial s_t}{\partial W_x}=x_t^T
$$

$$
\frac{\partial s_t}{\partial W_h}=h_{t-1}^T
$$

$$
\frac{\partial s_t}{\partial x_t}=W_x^T
$$

$$
\frac{\partial s_t}{\partial h_{t-1}}=W_h^T
$$

- 根据这些公式和梯度的链式法则，我们可以写出一个隐层单元中的反向传播过程，用一个函数`rnn_step_backward`来表示

```python
def rnn_step_backward(dnext_h, cache):
    """Backward pass for a single time step of a vanilla RNN.

    Inputs:
    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)
    - cache: Cache object from the forward pass

    Returns a tuple of:
    - dx: Gradients of input data, of shape (N, D)
    - dprev_h: Gradients of previous hidden state, of shape (N, H)
    - dWx: Gradients of input-to-hidden weights, of shape (D, H)
    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)
    - db: Gradients of bias vector, of shape (H,)
    """
    dx, dprev_h, dWx, dWh, db = None, None, None, None, None
    x, prev_h, Wx, Wh, b, next_h = cache
    ds = (1 - next_h ** 2) * dnext_h  # ds的大小是N*H
    db = np.sum(ds, axis=0)
    dWh = np.matmul(prev_h.T, ds)
    dWx = np.matmul(x.T, ds)
    dprev_h = np.matmul(ds, Wh.T)
    dx = np.matmul(ds, Wx.T)
    return dx, dprev_h, dWx, dWh, db
```

- 而RNN整体的反向传播也需要一个序列来完成，同时考虑从输出层的loss函数传递下来的梯度和从后一个隐藏单元传递下来的梯度，并使用函数`rnn_step_backward`来完成单个隐状态的更新。

```python
def rnn_backward(dh, cache):
    """
    Inputs:
    - dh: Upstream gradients of all hidden states, of shape (N, T, H)
    NOTE: 'dh' contains the upstream gradients produced by the 
    individual loss functions at each time step, *not* the gradients
    being passed between time steps (which you'll have to compute yourself
    by calling rnn_step_backward in a loop).
    """
   	N, T, H = dh.shape
    D = cache[0][0].shape[1]
    dx = np.zeros((N, T, D))
    dh0 = np.zeros((N, H))
    dWx = np.zeros((D, H))
    dWh = np.zeros((H, H))
    db = np.zeros((H, ))
    dh_prev = np.zeros((N, H))
    for i in range(T - 1, -1, -1):
        # 这一步特别重要，将两个方向传递过来的梯度融合
        dh_step = dh[:, i, :] + dh_prev
        dx_step, dh_prev, dwx, dwh, db_step = rnn_step_backward(dh_step, cache[i])
        dx[:, i, :] = dx_step
        dh0 = dh_prev
        dWx += dwx
        dWh += dwh
        db += db_step
    return dx, dh0, dWx, dWh, db
```

## 4.RNN的缺点

- 从上面的代码实现中可以看出，RNN一个很大的缺点就是对于一个序列必须串行化训练，不能并行训练，造成训练的速度非常缓慢，同时RNN也存在梯度爆炸和梯度消失的问题，对于**长距离的依赖关系也缺乏学习的能力**，这些问题的具体分析可以在CS224N的相关笔记中看到
- 拥有门控单元的RNN可以在一定程度上解决这个问题，比如LSTM