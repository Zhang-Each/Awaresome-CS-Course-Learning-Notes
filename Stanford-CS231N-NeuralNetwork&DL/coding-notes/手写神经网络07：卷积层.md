# 手写神经网络7：卷积层

> 这个系列是我在完成斯坦福公开课CS231N的作业时候所做的一些记录，这门课程是公认的深度学习入门的神课，其作业也非常硬核，需要从底层实现各种神经网络模型，包括前馈神经网络、卷积神经网络和循环神经网络，LSTM等等。
>
> 作业不仅要求掌握numpy等python库的api用法，更要对神经网络的数学理论和公式推导有非常深的理解，实现起来难度比较大，因此我也将在这个系列的博客中记录自己推导和coding的过程
>
> 限于个人水平实在有限，我尽量减少参考网上代码的次数

## 1.什么是卷积层？

### 1.1卷积的计算公式

- 卷积层是CNN中最核心的一个idea，通过卷积核(实质上就是一种滤波器)对输入的特征进行卷积运算，如果输入的数据是语音，那么数据是一维的，就可以进行一维卷积，如果输入的是矩阵，那么就可以进行二维的卷积，而CNN一般是用来处理图像的，图像是一种三维的数据(因为图像有颜色通道，每个通道上是一个二维矩阵，常见的RGB图像就是3颜色通道)，就需要用三维的卷积核对输入进行卷积操作。
- 假设一个卷积核的大小是$2M\times 2N$，那么 卷积的计算公式如下：

$$
s\left[i,j\right]=(x*w)[i,j]=\sum_{m=-M}^M\sum_{n=-N}^Nx[i+m,j+n]w[m,n]
$$

![dl2](static/dl2.png)

- 比如上面这张图中，一个大小为32x32x3的图像使用一个5x5x3的卷积核进行卷积之后得到的就是28x28x3的新图像，我们称卷积之后得到的新的图像称为feature map

### 1.2步长Stride

- 卷积核的移动存在一个步长(stride)，卷积核移动的步长小于卷积核的边长时，变会出现卷积核与原始输入矩阵作用范围在区域上的**重叠**，卷积核移动的步长与卷积核的边长相一致时，不会出现重叠现象
- 因此卷积操作需要设定一个步长，步长也会决定得到的新的feature map的大小

### 1.3接受域Receptive Field

- 卷积核可以覆盖到的局部特征区域，比如一个3x3的卷积核可以包含一个3x3区域内的信息，那么其接收域就是3x3，随着层数的加深，接受域也在变大。大的步长也使得接受域的增大速度变快

### 1.4Padding操作

- 我们发现按照卷积的运算方式，处于图像的边缘处的像素点可能没有对应位置的像素点来计算其卷积(也就是说用上面的公式进行计算的时候，下标溢出了)，padding操作可以解决这一问题

### 1.5卷积层带来的突破

- 《深度学习（花书）》中提出卷积这一操作的引入带来的突破有：
  - 稀疏交互Sparse interactions
    - 传统的神经网络使用矩阵乘法来建立输入输出的连接关系，参数矩阵中的每一个参数都代表了输入和输出的交互关系，而卷积层的运算因为卷积核的大小一般是远小于图像的大小的，因此只需要较少的计算量就可以提取关键的图像信息(比如图像的边缘)
    - 传统情况下，如果有m维度的输入和n维度的输出，那么参数矩阵就需要$m\times n$的规模，这种时候其实就是一种输入到输出的全连接，**通过卷积运算，减少了从输入到输出的连接数量**
  - 参数共享Parameter Sharing
    - 参数共享是说可以在一个模型的多个函数中使用一样的参数，传统的神经网络中，当计算了一层的输入的时候，权重矩阵的每一个元素只能使用一次，也就是说网络中有绑定的权重
    - 在CNN中，核的每个元素都作用在输入的每一个位置上，卷积运算的参数共享保证了只需要学习一个参数集合而不是对于每一个位置都需要学习一个单独的参数集合。
  - 等变表示Equivariant Representation
    - 如果一个函数满足输入改变，输出也以同样的方式改变，那么就可以称这个函数是等变的，如果对于函数f和g有$f(g(x))=g(f(x))$那么就称f和g具有等变性。
    - 对于卷积而言，参数共享的特殊形式是的神经网络层具有对平移操作等变的性质，这里可以令g为平移函数，那么g就可以表示图像函数的变换函数。
    - 简而言之这一性质表明，对于图像中一些需要提取的特征，**即使图像发生了平移，这个特征依然存在只是发生了对应的平移而已，仍然可以用一样的方式提取出来**。在处理时间序列数据的时候，这个性质意味着卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。

## 2.卷积层的代码实现

- 下面使用一个函数`conv_forward_naive`来实现一种非常naive的卷积层计算方法，事实上就是用多重循环将卷积的结果一个个算出来。注意这里的卷积运算有个bias，上面的公式没有提到

```python
def conv_forward_naive(x, w, b, conv_param):
		"""
    - x: Input data of shape (N, C, H, W)
    - w: Filter weights of shape (F, C, HH, WW)
    - b: Biases, of shape (F,)
    Returns a tuple of:
    - out: Output data, of shape (N, F, H', W') where H' and W' are given by
      H' = 1 + (H + 2 * pad - HH) / stride
      W' = 1 + (W + 2 * pad - WW) / stride
    - cache: (x, w, b, conv_param)
    """
    out = None
    stride = conv_param["stride"]
    pad = conv_param["pad"]
    N, C, H, W = x.shape
    F, C, HH, WW = w.shape
    new_H, new_W = int(1 + (H + 2 * pad - HH) / stride), int(1 + (W + 2 * pad - WW) / stride)
    out = np.zeros((N, F, new_H, new_W))
    # 对x进行zero-pad操作，得到一个扩充后的矩阵，这里只对x的第三四个维度(也就是像素值所在的维度)进行扩充
    x_pad = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant', constant_values=(0, 0))
    for n in range(N):
        new_x = x_pad[n]
        for f in range(F):
            # 得到卷积核
            kernel = w[f]
            for i in range(new_H):
                for j in range(new_W):
                    # 取出对应的卷积区域，并进行计算
                    conv_zone = new_x[:, i * stride: i * stride + HH, j * stride: j * stride + WW]
                    conv_res = np.sum(conv_zone * kernel) + b[f]
                    out[n, f, i, j] = conv_res
    cache = (x, w, b, conv_param)
    return out, cache

```

- 当然这种实现方式是非常naive的，只能用来测试自己是否理解了卷积操作，真的用来构建神经网络的话计算效率会非常低，CS231N的assignment2中给出了cpython加速的卷积操作的向量化实现，可以较大地提高计算的效率。

## 3.卷积层的反向传播

- 卷积的反向传播比较简单，我们假设卷积核的大小是$2M\times 2N$，那么对于卷积后得到的feature map的一个像素点$s_{ij}$，我们根据上面的卷积公式可以得到：

$$
\frac{\partial s_{ij}}{\partial x_{i+m,j+n}}=w_{mn}，m\in [-M,M],n\in [-N,N]
$$

$$
\frac{\partial s_{ij}}{\partial w_{mn}}=x_{i+m,j+n},，m\in [-M,M],n\in [-N,N]
$$

$$
\frac{\partial s_{ij}}{\partial b}=1^{2M\times 2N}
$$

- 因此反向传播求dx的时候只要把feature map的每个像素点进行遍历，把权重加到原位置的点上就可以，并且还要乘上传递到这一层的梯度dout，具体的代码实现如下：

```python
def conv_backward_naive(dout, cache):
    """A naive implementation of the backward pass for a convolutional layer.
    Inputs:
    - dout: Upstream derivatives.
    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive
    Returns a tuple of:
    - dx: Gradient with respect to x
    - dw: Gradient with respect to w
    - db: Gradient with respect to b
    """
    dx, dw, db = None, None, Nonex, w, b, conv_param = cache
    stride = conv_param.get('stride', 1)
    pad = conv_param.get('pad', 0)
    # 定义好梯度的大小
    dx = np.zeros(x.shape, dtype=np.float64)
    dw = np.zeros(w.shape, dtype=np.float64)
    db = np.zeros(b.shape, dtype=np.float64)
    N, C, H, W = x.shape
    F, C_prime, HH, WW = w.shape
    H_prime = int(1 + (H + 2 * pad - HH) / stride)
    W_prime = int(1 + (W + 2 * pad - WW) / stride)
    # 进行padding操作
    dx_pad = np.pad(dx, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant', constant_values=(0, 0))
    x_pad = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant', constant_values=(0, 0))

    for i in range(N):
        x_i = x_pad[i]
        for j in range(F):
            w_j = w[j]
            for h_now in range(H_prime):
                for w_now in range(W_prime):
                    # 在原来的对应像素位置上，按照权重加上梯度
                    db[j] += dout[i, j, h_now, w_now]
                    dx_pad[i, :, h_now * stride:h_now * stride + HH, w_now * stride:w_now * stride + WW] += dout[
                                                                                                                i, j, h_now, w_now] * w_j
                    dw[j] += dout[i, j, h_now, w_now] * x_i[:, h_now * stride:h_now * stride + HH,
                                                        w_now * stride:w_now * stride + WW]

    dx = dx_pad[:, :, pad:-pad, pad:-pad]
    return dx, dw, db
```

