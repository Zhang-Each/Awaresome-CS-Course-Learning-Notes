# 隐马尔可夫模型(HMM)

## 模型的定义

隐马尔可夫模型(**Hidden Markov Model, HMM**)是一种描述由隐藏的马尔可夫链生成观测序列的模型，这里的序列可以是时间序列，也可以文本序列等等，一个马尔可夫链可以生成一个随机状态的序列，而这些状态被隐藏不能直接观察(因此称为隐状态)，但是每个状态以一定的概率表现出若干种可能的观测结果，隐马尔可夫模型就是用来描述和建模这类问题的。

我们假设所有可能的隐状态构成的集合是$Q=(q_1,q_2,\dots,q_N)$，而所有可能观测到结果的集合是$V=(v_1,v_2,\dots,v_M)$，即一共有N种隐状态和M种不同的观测结果。同时我们假设要研究的序列长度为T，其隐状态和观测结果序列分别是$I=(i_1,i_2,\dots,i_T),O=(o_1,o_2,\dots,o_T)$

同时各个状态之间会以一定的概率进行转移(HMM假设一个状态只依赖于前一个状态，即隐马尔可夫模型中只存在**一阶马尔可夫链**)，而每个状态又以一定的概率生成各种不同的观测结果，我们假设状态转移矩阵$A=[a_{ij}]_{N\times N}$，观测概率矩阵$B=[b_j(k)]_{N\times M}$,其中：
$$
a_{ij}=P(i_{i+1}=q_j|i_t=q_i),\quad b_j(k)=P(o_t=v_k|i_t=q_j)
$$
另外用$\pi=(\pi_i)$来表示初始概率向量，表示序列初始的时候处于各个可能状态的概率。

状态转移矩阵A和初始概率向量确定了隐藏的马尔可夫链，而观测概率矩阵确定了如何从状态生成观测，因此三者可以共同确定一个马尔可夫模型，可以用三要素$\lambda=(A,B,\pi)$来表示一个隐马尔可夫模型，同时隐马尔可夫模型还有如下假设：

- 隐藏的马尔可夫链在任何时刻的状态只依赖于前一个位置的状态
- 任何时候观测到的结果只和当前的状态有关，和其他时刻的状态以及观测到的结果无关

隐马尔可夫模型可以解决这样几种问题：

- 根据已知的HMM来计算某个序列O出现的概率
- 根据观测到的序列O来估计模型的各个参数$\lambda$使得该模型下观测到该序列的概率$P(O|\lambda)$最大，换句话说就是极大似然估计
- 对于给定的HMM和观测序列O，求出最有可能的状态序列I，这实际上就是一个解码问题

## 概率计算：前向和后向算法

第一个任务是计算给定模型$\lambda$条件下某个序列O出现的概率，很明显我们可以用条件概率的公式计算，先考虑所有可能的状态序列，再对所有可能的状态序列产生观测序列O的概率进行求和，即：
$$
P(O|\lambda)=\sum_I P(O,I|\lambda)P(I|\lambda)
$$
而状态序列$I=(i_1,i_2,\dots,i_T)$在模型中出现是：
$$
P(I|\lambda)=\pi_{i_1}\prod_{j=1}^{T-1}a_{i_ji_{j+1}}
$$
而对于固定的状态序列I出现观测序列O的概率是：
$$
P(O|I,\lambda)=\prod_{j=1}^Tb_{i_j}(o_j)
$$

### 前向算法

前向算法是给定HMM，并且定义到t时刻为止的序列为$o_1,o_2,\dots,o_t$且当前隐状态为$q_i$的概率记为：
$$
\alpha_t(i)=P(o_1,o_2,\dots,o_t,i_t=q_t|\lambda)
$$
前向算法提供了一种动态规划的方式来一个个计算$\alpha_t(i)$，对于t+1时刻，如果当前的隐状态是$q_i$那么这一事件发生的概率也就是$\alpha_{t+1}(i)$，而这一情况可以有不同的t时刻的情况转化而来，t时刻所有可能的情况$\alpha_t(j)$可以有$a_{ji}$的概率转化成$\alpha_{t+1}(i)$，而状态$q_i$表现出观测状态$o_{t+1}$的概率就是$b_i(o_{t+1})$，因此$\alpha_{t+1}(i)$可以表示为：
$$
\alpha_{t+1}(i)=\left[\sum_{j=1}^N\alpha_t(j)a_{ji}\right]b_i(o_{t+1})
$$
最终要计算的$P(O|\lambda)$可以表示为T时刻所有可能的隐状态表现出$o_T$的概率之和，即：
$$
P(O|\lambda)=\sum_{i=1}^N\alpha_{t+1}(i)=\sum_{i=1}^N\left[\sum_{j=1}^N\alpha_t(j)a_{ji}\right]b_i(o_{t+1})
$$
很显然前向算法就是用动态规划简化了HMM中的概率计算问题，我们很容易根据初始值$\alpha_1(i)=\pi_i b_i(o_1)$来计算。

### 后向算法

后向算法定义在t时刻隐状态为$q_i$的条件下，从t+1到T时刻的观测序列为$(o_{t+1},o_{t+2},\dots,o_T)$的概率为：
$$
\beta_t(i)=P(o_{t+1},o_{t+2},\dots,o_T|\lambda, i_t=q_i)
$$
然后就可以从$\beta_T(i)=1$开始逐步向前计算，我们发现每个$\beta_t(i)$都有一定的概率转化成不同的$\beta_{t+1}(j)$，而每个t+1时刻的隐状态j又可以以一定的概率转化成观测结果$o_{t+1}$，可以得到：
$$
\beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}(j)
$$
后向算法实际上就是一种反推，我们可以将要求的结果$P(o|\lambda)$表示为t=1的时候所有的可能性的和：
$$
P(O|\lambda )=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)=\sum_{i=1}^N\sum_{j=1}^N\alpha_{t}(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
$$

- 还可以将前向后向概率进行结合来计算$P(O|\lambda)$

## 模型学习算法

### 监督学习

如果说我们观测到了一系列长度相同的观测序列$O_1,O_2,\dots ,O_S$和对应的隐状态序列$I_1,I_2,\dots,I_S$，那么这个时候问题就变成了一个**监督学习的问题**，就比较容易可以求解了，我们可以使用极大似然法直接估计出隐马尔可夫模型的参数$\lambda =(A,B,\pi)$

首先来估计状态转移概率，我们可以直接使用样本中所有从t时刻状态i转移到t+1时刻的状态j的频数$A_{ij}$来估计所有的状态转移概率：
$$
a_{ij}=\frac{A_{ij}}{\sum_{j=1}^NA_{ij}}
$$
而观测概率$b_j(k)$也可以使用样本中所有状态为j并且观测结果为k的频数来估计：
$$
b_j(k)=\frac{B_{jk}}{\sum_{i=1}^MB_{jk}}
$$
而初始状态直接使用样本中所有初始状态的分布情况作为估计即可。

### EM算法

假设没有观测到隐状态序列，而只观察到了观测序列$O_1,O_2,\dots ,O_S$那么就需要用EM算法来估计模型的参数。

- 我先来看看EM算法的内容，这一块内容感觉挺难顶的

## 预测算法