# PyTorch学习笔记2：用GPU加速计算

![pytorch](https://pytorch.apachecn.org/docs/img/logo.svg)

> 最近在阿里云暑期摸鱼实习，分到了一台有4个GPU的服务器让我先试试在服务器上做一些CUDA相关的实验，于是在配置了一通开发环境之后终于勉强能用了，下面放一些用使用GPU和CPU分别进行矩阵计算时候的性能对比来探究一下GPU对于矩阵计算的加速能力。

## GPU和CPU的异构计算

​	  这一部分在另一个目录intern下有关于GPU/CUDA更详细的叙述，总的来说GPU体系结构的设计对计算密集型任务进行了一定的特化，因此对计算密集型的任务处理效率特别高，而对于一些复杂的逻辑处理还是应该依赖CPU，因此也就形成了CPU-GPU的异构计算体系。本文主要通过一些实验来探究PyTorch中调用GPU带来的计算加速效果。

## PyTorch如何调用GPU

​	  PyTorch集成了CUDA相关的功能，当前计算机是否可以调用GPU可以用`torch.cuda.is_available()`方法判断，同时PyTorch中的张量可以用`cuda()`方法从CPU中转移到GPU中进行计算，也可以用`to(device)`方法在CPU和GPU之间任意地切换，被转移到GPU中的张量相关的计算就会在GPU中进行。

## 实验

​	  我编写了一些实验代码，用服务器上的CPU和GPU分别进行了指定维度的随机矩阵的加法和乘法运算，并对GPU和CPU的运算效率进行了实验对比，其中具体的实验分组如下：

- 迭代次数有100，1000，2000，5000，8000，10000，15000，20000等几种
- 矩阵的维数(n*n)有16，32，64，128，256，512，1024，2048，4096等几种选择

同时我们计算了CPU运算时间和GPU运算所需时间的比值，得到了如下实验结果：

待补充