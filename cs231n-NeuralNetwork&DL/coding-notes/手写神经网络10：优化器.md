# 手写神经网络10：优化器

> 这个系列是我在完成斯坦福公开课CS231N的作业时候所做的一些记录，这门课程是公认的深度学习入门的神课，其作业也非常硬核，需要从底层实现各种神经网络模型，包括前馈神经网络、卷积神经网络和循环神经网络，LSTM等等。
>
> 作业不仅要求掌握numpy等python库的api用法，更要对神经网络的数学理论和公式推导有非常深的理解，实现起来难度比较大，因此我也将在这个系列的博客中记录自己推导和coding的过程
>
> 限于个人水平实在有限，我尽量减少参考网上代码的次数

## 1.神经网络的训练

- 神经网络的参数学习的总原则是用反向传播求出梯度，然后用梯度下降的方法对参数进行更新，而对于神经网络的训练的主要改进，就是对梯度下降的方法进行改进，最直接的批梯度下降会带来巨大的运算量，而**随机梯度下降**的出现解决了直接对整个数据集进行梯度下降求解时计算量过大的问题。
- 前面在《手写神经网络4》中我们已经实现了一个SGD优化器对神经网络中的参数进行随机梯度下降的优化，
  - 而SGD虽然相比于梯度下降**对训练速度有了比较大的提升**，但是计算出的梯度受数据的影响比较大(因为只有一个小批量的数据)，可能会因为数据分布的不均匀导致梯度变化过大，不利于逼近最优点
  - 对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解
  - 随机梯度下降的计算密度也比较高，仍然可以继续改进
- 因此在SGD的基础上，衍生出了若干种改进的训练方法
  - 动量随机梯度下降
  - RMSProp算法
  - Adam算法

## 2.动量梯度下降法

- 动量梯度下降法的提出是为了加快学习的速度，特别是处理高曲率，小但是一致的梯度，动量方法会对之前已经求导的梯度进行指数级别的加权平均，并继续沿着该方向移动，其具体的公式为：

$$
V\leftarrow\alpha V-\epsilon V_{batch}
$$

$$
\theta \leftarrow\theta+V
$$

- 其代码实现也比较容易，按照公式一步步计算就可以

```python
def sgd_momentum(w, dw, config=None):
    """
    Performs stochastic gradient descent with momentum.
    config format:
    - learning_rate: Scalar learning rate.
    - momentum: Scalar between 0 and 1 giving the momentum value.
      Setting momentum = 0 reduces to sgd.
    - velocity: A numpy array of the same shape as w and dw used to store a
      moving average of the gradients.
    """
    if config is None:
        config = {}
    config.setdefault("learning_rate", 1e-2)
    config.setdefault("momentum", 0.9)
    v = config.get("velocity", np.zeros_like(w))
    # 公式原文
    next_w = None
    v = v * config["momentum"] - config["learning_rate"] * dw
    next_w = w + v;
    config["velocity"] = v
    return next_w, config
```



## 3.RMSProp算法

- RMSProp是一种自适应学习率的算法，在AdaGrad算法的基础上进行改进，改变梯度累积为指数加权平均，在凸问题的求解中可以快速收敛，其具体的学习方式是：
- 先计算梯度并求出累计的平方梯度

$$
g=\frac 1m\nabla_{\theta}\sum_{i}L(f(x_i),y_i)
$$

$$
r\leftarrow \rho r+(1-\rho)g\odot g
$$

- 更新参数：

$$
\theta \leftarrow\theta-\frac{\epsilon}{\sqrt{\delta +r}}\odot g
$$

- 其代码实现如下，每一步可以和公式一一对应：

```python
def rmsprop(w, dw, config=None):
    """
    Uses the RMSProp update rule, which uses a moving average of squared
    gradient values to set adaptive per-parameter learning rates.
    config format:
    - learning_rate: Scalar learning rate.
    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared
      gradient cache.
    - epsilon: Small scalar used for smoothing to avoid dividing by zero.
    - cache: Moving average of second moments of gradients.
    """
    if config is None:
        config = {}
    config.setdefault("learning_rate", 1e-2)
    config.setdefault("decay_rate", 0.99)
    config.setdefault("epsilon", 1e-8)
    config.setdefault("cache", np.zeros_like(w))
    next_w = None
    dr = config["decay_rate"]
    cache = dr * config["cache"] + (1 - dr) * (dw ** 2)
    config["cache"] = cache
    next_w = w - config["learning_rate"] * dw / (np.sqrt(cache) + config["epsilon"])
    return next_w, config
```



## 4.Adam算法

- Adam算法是另一种具有自适应学习率的算法，在自适应学习率中引入了动量，对梯度进行一阶和二阶的有偏矩估计

$$
s=\frac{\rho_1s+(1-\rho_1)g}{1-\rho_1^2}
$$

$$
r=\frac{\rho_2r+(1-\rho_2)r}{1-\rho_2^2}
$$

$$
\theta\leftarrow\theta-\epsilon \frac{s}{\sqrt{r+\delta}}
$$

- 这几个优化器的代码实现其实大同小异，基本都是一个套路，关键在于看懂公式究竟在算什么东西

```python
def adam(w, dw, config=None):
    """
    Uses the Adam update rule, which incorporates moving averages of both the
    gradient and its square and a bias correction term.
    config format:
    - learning_rate: Scalar learning rate.
    - beta1: Decay rate for moving average of first moment of gradient.
    - beta2: Decay rate for moving average of second moment of gradient.
    - epsilon: Small scalar used for smoothing to avoid dividing by zero.
    - m: Moving average of gradient.
    - v: Moving average of squared gradient.
    - t: Iteration number.
    """
    config["t"] += 1
    config["m"] = config["beta1"] * config["m"] + (1 - config["beta1"]) * dw
    mt = config["m"] / (1 - config["beta1"] ** config["t"])
    config["v"] = config["beta2"] * config["v"] + (1 - config["beta2"]) * (dw ** 2)
    vt = config["v"] / (1 - config["beta2"] ** config["t"])
    next_w = w - config["learning_rate"] * mt / (np.sqrt(vt) + config["epsilon"])
    return next_w, config
```