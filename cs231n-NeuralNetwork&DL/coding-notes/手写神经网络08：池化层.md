# 手写神经网络8：池化层

> 这个系列是我在完成斯坦福公开课CS231N的作业时候所做的一些记录，这门课程是公认的深度学习入门的神课，其作业也非常硬核，需要从底层实现各种神经网络模型，包括前馈神经网络、卷积神经网络和循环神经网络，LSTM等等。
>
> 作业不仅要求掌握numpy等python库的api用法，更要对神经网络的数学理论和公式推导有非常深的理解，实现起来难度比较大，因此我也将在这个系列的博客中记录自己推导和coding的过程
>
> 限于个人水平实在有限，我尽量减少参考网上代码的次数

## 1.什么是池化层

- 当我们发现了一个特征之后，这个特征相对于其他特征的位置相比于这个特征的值而言更为重要，很明显的一个例子就是计算机视觉中的目标检测任务，我们要检测的目标所处周围环境的特点更能帮助我们检测出真的目标，这就是池化层的idea的来源，池化层是CNN架构中提出的另一种特殊层，起到了**降采样**的作用。
- 池化就是通过一种非线性的变换，对特征进行进一步的**抽象和降维**，常见的有最大池化，平均池化等等，就是按照池化窗口的大小将矩阵分成若干个区域，在每个区域中进行池化操作(求最大，均值等等)，生成一个维度更小的feature map，同时池化层也可以起到防止过拟合的作用。
- 池化层同样也有长宽和步长等参数

## 2.池化层的前向传播

- 池化层的前向传播也是通过一系列for循环进行对应的池化操作，这里我们以最大池化为例，实现了函数`max_pool_forward_naive` 

```python
def max_pool_forward_naive(x, pool_param):
    """A naive implementation of the forward pass for a max-pooling layer.
    Inputs:
    - x: Input data, of shape (N, C, H, W)
    - pool_param: dictionary with the following keys:
      - 'pool_height': The height of each pooling region
      - 'pool_width': The width of each pooling region
      - 'stride': The distance between adjacent pooling regions
    No padding is necessary here, eg you can assume:
      - (H - pool_height) % stride == 0
      - (W - pool_width) % stride == 0
    Returns a tuple of:
    - out: Output data, of shape (N, C, H', W') where H' and W' are given by
      H' = 1 + (H - pool_height) / stride
      W' = 1 + (W - pool_width) / stride
    - cache: (x, pool_param)
    """
    out = None
    N, C, H, W = x.shape
    h, w, stride = pool_param["pool_height"], pool_param["pool_width"], pool_param['stride']
    new_H, new_W = int(1 + (H - h) / stride), int(1 + (W - w) / stride)
    out = np.zeros((N, C, new_H, new_W))
    for n in range(N):
        for c in range(C):
            for i in range(new_H):
                for j in range(new_W):
                    # 按块取出最大值即可
                    max_pix = np.max(x[n, c, i * stride: i * stride + h, j * stride: j * stride + w])
                    out[n, c, i, j] = max_pix
    cache = (x, pool_param)
    return out, cache
```

## 3.池化层的反向传播

- 反向传播涉及到池化层的求导问题，这里我们使用的是最大池化，因此也就是需要对max函数进行求导，而在ReLU层我们已经知道max函数的导数是分段的，小于0的部分是0，大于0的部分是1，而在池化层中也是一样的道理，对于进行池化操作的每个区域，最大的那个位置的梯度就是池化层向后传递的梯度，而其他地方都是0，我们可以在cache中记录原本矩阵的信息，并在池化层的反向传播过程中使用。
- 当然如果一个区域内有多个点的都是最大值的时候，可以把梯度进行平均分，这一过程在`max_pool_backward_naive`中通过一个`mask`矩阵来实现：

```python
def max_pool_backward_naive(dout, cache):
    """A naive implementation of the backward pass for a max-pooling layer.
    Inputs:
    - dout: Upstream derivatives
    - cache: A tuple of (x, pool_param) as in the forward pass.
    Returns:
    - dx: Gradient with respect to x
    """
    dx = None
    x, pool_param = cache
    h, w, stride = pool_param["pool_height"], pool_param["pool_width"], pool_param['stride']
    N, C, H, W = x.shape
    new_H, new_W = int(1 + (H - h) / stride), int(1 + (W - w) / stride)
    dx = np.zeros(x.shape)
    for n in range(N):
        for c in range(C):
            for i in range(new_H):
                for j in range(new_W):
                    pool = x[n, c, i * stride: i * stride + h, j * stride: j * stride + w]
                    mask = np.zeros(pool.shape)
                    mask[pool == np.max(pool)] = 1
                    mask /= np.sum(mask)
                    dx[n, c, i * stride: i * stride + h, j * stride: j * stride + w] = mask * dout[n, c, i, j]
		return dx
```

