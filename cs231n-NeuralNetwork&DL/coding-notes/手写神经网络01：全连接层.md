# 手写神经网络1：全连接层

> 这个系列是我在完成斯坦福公开课CS231N的作业时候所做的一些记录，这门课程是公认的深度学习入门的神课，其作业也非常硬核，需要从底层实现各种神经网络模型，包括前馈神经网络、卷积神经网络和循环神经网络，LSTM等等。
>
> 作业不仅要求掌握numpy等python库的api用法，更要对神经网络的数学理论和公式推导有非常深的理解，实现起来难度比较大，因此我也将在这个系列的博客中记录自己推导和coding的过程。

神经网络又输入层输出层和中间的若干个隐层组成，这些隐层就是构成神经网络的基本组件。全连接层Fully Connected Layer是最简单的一种神经网络组件，也叫做仿射层。

## 1.1正向传播

全连接层也就是**将所有的输入神经元和所有输出神经元进行连接**，彼此之间有一个权重，我们假设输入神经元的是$d$维向量$x$，输出的是$n$维的向量，那么全连接层的权重矩阵$W$就是一个$n\times d$维的矩阵，而bias就是一个$n$维的向量$b$，因此全连接层的正向传播的过程可以表示为：
$$
f(x)=Wx+b
$$
这一过程时间上就是对输入的向量x进行了一个线性的变换。这一部分比较简单，也没什么好多说的。具体的代码实现如下，用一个函数`affine_forward`来表示一个全连接层前向传播时的操作：

```python
def affine_forward(x, w, b):
    """
    Computes the forward pass for an affine (fully-connected) layer.
    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
    examples, where each example x[i] has shape (d_1, ..., d_k). We will
    reshape each input into a vector of dimension D = d_1 * ... * d_k, and
    then transform it to an output vector of dimension M.
    Inputs:
    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)
    - w: A numpy array of weights, of shape (D, M)
    - b: A numpy array of biases, of shape (M,)
    Returns a tuple of:
    - out: output, of shape (N, M)
    - cache: (x, w, b)
    """
    out = None
    D = np.prod(x.shape[1: ])
    new_x = x.reshape((x.shape[0], D))
    out = np.matmul(new_x, w) + b
    cache = (x, w, b)
    return out, cache
```



## 1.2反向传播

反向传播也就是根据损失函数来优化各个参数W，x和b，我们可以假设从输出层传递到当前全连接层的结果是dout，那么根据反向传播算法和梯度的链式法则，我们需要求出的就是函数f对于W，x和b三者的导数。
$$
\frac {\partial f}{\partial x}=W^T
$$

$$
\frac {\partial f}{\partial b}=\bold 1^{d\times 1}
$$

$$
\frac {\partial f}{\partial W}=x^T
$$

**结合W，x，b各自的维度特征**，反向传播的这个过程可以用下面的函数`affine_backward`来表示：

```python
def affine_backward(dout, cache):
    """
    Computes the backward pass for an affine layer.
    Inputs:
    - dout: Upstream derivative, of shape (N, M)
    - cache: Tuple of:
      - x: Input data, of shape (N, d_1, ... d_k)
      - w: Weights, of shape (D, M)
      - b: Biases, of shape (M,)
    Returns a tuple of:
    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
    - dw: Gradient with respect to w, of shape (D, M)
    - db: Gradient with respect to b, of shape (M,)
    """
    x, w, b = cache
    dx, dw, db = None, None, None
    N, D = x.shape[0], w.shape[0]
    dx = np.matmul(dout, w.T).reshape(x.shape)
    dw = np.matmul(x.reshape(N, D).T, dout)
    db = np.sum(dout, axis=0)
    return dx, dw, db

```

事实上我也没搞懂为什么导数会是这样的形式，但是根据矩阵和向量的维度来看应该需要这么写。