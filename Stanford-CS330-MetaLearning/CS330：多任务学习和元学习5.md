# CS330：多任务学习和元学习5

> Stanford CS330多任务学习和元学习的Lecture8，主题是贝叶斯元学习，这节课的主要内容是在说如何将元学习解释成贝叶斯推理，并且理解如何使用参数来表示问题的不确定性。



## 贝叶斯元学习

上节课的内容提到了元学习算法最重要的能力是表示能力和一致性(Consistency，我不知道翻译成什么比较好，或许也可以翻译成相容性)，表示能力也就是学习到的函数可以表示一系列学习过程和任务的能力，而Consistency则表示一个算法学习到了“如何学习”的过程并用足够多的数据来解决问题的能力。今天则提出了第三个评价标准——对不确定性的感知能力(Uncertainty Awareness)，也就是在学习的过程中对模棱两可的情况进行推理的能力

### 为什么需要贝叶斯

多任务学习和元学习有一些基本的准则，那就是需要有多个任务进行训练，同时这些任务需要有相似的结构，相似的结构指的就是这些任务会有一些共通的潜在信息$\theta$，这些潜在信息在不同的任务中可能有不同的表现形式，比如在一个玩具级的信号拟合任务中可能代表了不同的信号函数需要有相同的变化规律(周期等等)，而在一个多语言机器翻译的任务中可能是指不同的语言有一些共通的语法.

在参数化的方法中，我们可以用$p(\phi_i|D_i^{tr},\theta)$来表示一个元学习模型，在少样本学习的场景下，这种学习方式可能是模棱两可的，因此就需要用贝叶斯来进行推理。

### 贝叶斯元学习算法

- 为了解决上面的问题，一个很直观的想法是训练模型使得函数f可以**输出测试集的概率分布参数**

贝叶斯元学习算法需要在原有的参数化方法上进行扩展，也就是在黑盒学习和优化学习模型中引入贝叶斯，黑盒学习模型中引入贝叶斯可以表示为：

![image-20210807100459560](static/image-20210807100459560.png)

很抽象，我也没看懂，就当图个乐好了。

基于优化的方法也有一堆公式推导，看不懂，先放弃了。



### 如何评估贝叶斯元学习器

- 使用合适和metric，具体任务具体分析(PPT中举出了很多案例)