# 论文阅读笔记16：BERT

> BERT预训练模型原论文阅读笔记，了解一下BERT的基本架构和设计

## Introduction

BERT(**Bidirectional Encoder Representation from Transformer**，Transformer双向编码表示)是Google发布的一个预训练语言模型，可以从双向的上下文信息中获得双向的token编码表示，同时可以通过添加新层并进行微调训练之后在各种NLP任务上达到SOTA的效果。

BERT的结构很简单但是效果很强大(~~因为训练烧的钱也多~~)，并且在十一种NLP子任务中都取得了非常好的表现。

### 语言模型和预训练

语言模型的预训练已经被证明是非常有效的解决NLP任务的手段，预训练的语言模型在句子级的人物(自然语言推理，分段)和单词级的任务(命名实体识别，问答)中都表现的比较好。

预训练模型的使用方式一般有两种，一种是基于特征的方法(**feature-based**)，另一种是微调(**fine-tuning**)，基于特征的方法是**将从预训练好的模型中得到的特征**作为额外的特征输入，比如ELMo，而微调则是在训练好的模型基础上加上若干层和参数并再针对下游任务进行训练，两种方式在预训练的时候采用的是同样的目标函数，主要的区别在于如何解决下游问题。

作者认为现有的预训练方法对微调的限制过大，因为标准的语言模型都是单向的，这就限制了预训练的时候能够使用的网络架构，比如GPT模型在训练的时候使用的就是从左到右单方向的架构，并且每个token在计算注意力的时候只能和在它前面的token之间进行计算，这就给微调的方式带来了非常糟糕的影响，因为模型无法从两个方向来感知上下文了。而BERT解决了这个问题，提出了一种双向的预训练方式，使得模型更加适合通过微调的方式来解决下游任务。

